{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31b491c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\New folder\\Project\\ProjectData\\Data Science\\datascience\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import sys\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from transformers import pipeline\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10b32e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')  # สำหรับ NLTK 3.9 ขึ้นไป"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfdc678",
   "metadata": {},
   "source": [
    "6.1 : Cleaning Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65aa1734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Interrobang. By Aishwarya Henriette',\n",
       " 'Parking And Going. BY Karl Gautier',\n",
       " 'Today Is The night. By Jarek Prakash']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data = [\"  Interrobang. By Aishwarya Henriette  \",\n",
    "             \"Parking And Going. BY Karl Gautier\",\n",
    "             \"  Today Is The night. By Jarek Prakash  \",]\n",
    "\n",
    "strip_whitespace = [string.strip() for string in text_data]\n",
    "\n",
    "strip_whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30b659ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Interrobang By Aishwarya Henriette',\n",
       " 'Parking And Going BY Karl Gautier',\n",
       " 'Today Is The night By Jarek Prakash']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_periods  = [string.replace(\".\",\"\") for string in strip_whitespace]\n",
    "\n",
    "remove_periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83b8cabd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['INTERROBANG BY AISHWARYA HENRIETTE',\n",
       " 'PARKING AND GOING BY KARL GAUTIER',\n",
       " 'TODAY IS THE NIGHT BY JAREK PRAKASH']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def capitalizer(string: str) -> str:\n",
    "    return string.upper()\n",
    "\n",
    "[capitalizer(string) for string in remove_periods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02af5766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['XXXXXXXXXXX XX XXXXXXXXX XXXXXXXXX',\n",
       " 'XXXXXXX XXX XXXXX XX XXXX XXXXXXX',\n",
       " 'XXXXX XX XXX XXXXX XX XXXXX XXXXXXX']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def replace_letters_with_X(string: str) -> str:\n",
    "    return re.sub(r'[a-zA-Z]', 'X', string)\n",
    "\n",
    "[replace_letters_with_X(string) for string in remove_periods]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19d5f31",
   "metadata": {},
   "source": [
    "Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eef64c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5|True|False|False|False|b'machine learning in python cookbook'|machine learning in python cookbook\n"
     ]
    }
   ],
   "source": [
    "s = \"machine learning in python cookbook\"\n",
    "\n",
    "find_n = s.find(\"n\")\n",
    "\n",
    "start_with_m = s.startswith(\"m\")\n",
    "\n",
    "end_with_python = s.endswith(\"python\")\n",
    "\n",
    "is_alnum = s.isalnum()\n",
    "is_alpha = s.isalpha()\n",
    "\n",
    "encode_as_utf8 = s.encode(\"utf-8\")\n",
    "\n",
    "decode = encode_as_utf8.decode(\"utf-8\")\n",
    "\n",
    "print(find_n, start_with_m, end_with_python, is_alnum, is_alpha, encode_as_utf8, decode,sep = \"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae40f70",
   "metadata": {},
   "source": [
    "6.2 : Parsing and Cleaning HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aae8148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Masego Azra'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = \"<div class='full_name'>\"\\\n",
    "        \"<span style='font-weight:bold'>Masego\"\\\n",
    "        \"</span> Azra</div>\"\n",
    "\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "soup.find('div', {\"class\" : 'full_name'}).text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854fd210",
   "metadata": {},
   "source": [
    "6.3 : Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e138ae82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi I Love This Song', '10000 Agree LoveIT', 'Right']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data = [\"Hi!!!! I. Love. This. Song....\" , \n",
    "             '10000% Agree!!!! #LoveIT' ,\n",
    "             'Right?!?!']\n",
    "\n",
    "punctuatuation = dict.fromkeys(\n",
    "    (i for i in range(sys.maxunicode)\n",
    "     if unicodedata.category(chr(i)).startswith('P')\n",
    "     ),\n",
    "    None\n",
    "    )\n",
    "\n",
    "[String.translate(punctuatuation) for String in text_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655b17e5",
   "metadata": {},
   "source": [
    "6.4 : Tokenizing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b9e5631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'science',\n",
       " 'of',\n",
       " 'today',\n",
       " 'is',\n",
       " 'the',\n",
       " 'technology',\n",
       " 'of',\n",
       " 'tomorrow',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"The science of today is the technology of tomorrow.\"\n",
    "word_tokenize(string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e80a7e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The science of today is the technology of tomorrow.', 'Tomorrow is today.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"The science of today is the technology of tomorrow. Tomorrow is today.\"\n",
    "sent_tokenize(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52acfc1",
   "metadata": {},
   "source": [
    "6.5 : Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11be9bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['going', 'go', 'store', 'park']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_words = ['i',\n",
    "                   'am',\n",
    "                   'going',\n",
    "                   'to',\n",
    "                   'go',\n",
    "                   'to',\n",
    "                   'the',\n",
    "                   'store',\n",
    "                   'and',\n",
    "                   'park',\n",
    "                  ]\n",
    "stop_words = stopwords.words('english')\n",
    "[word for word in tokenized_words if word not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854c10bb",
   "metadata": {},
   "source": [
    "Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "533fbc33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'about', 'above', 'after', 'again']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cd9429",
   "metadata": {},
   "source": [
    "6.6 : Stemming Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc724ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'humbl', 'by', 'thi', 'tradit', 'meet']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_words = ['i','am','humbled','by','this','traditional','meeting']\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "[porter.stem(word) for word in tokenized_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659cfb84",
   "metadata": {},
   "source": [
    "6.7 : Tagging Parts of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d41be34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Chris', 'NNP'), ('loved', 'VBD'), ('outdoor', 'RP'), ('running', 'VBG')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data = \"Chris loved outdoor running\"\n",
    "text_tagged = pos_tag(word_tokenize(text_data))\n",
    "text_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5ef58d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chris']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word for word,tag in text_tagged if tag in ['NN' , 'NNS', 'NNP' , 'NNPS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6708ea67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 0, 1, 1, 1, 0],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 1, 1, 1, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = [\"I am eating a burrito for breakfast\",\n",
    " \"Political science is an amazing field\",\n",
    " \"San Francisco is an awesome city\"]\n",
    "\n",
    "tagged_tweets = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    tweet_tag = nltk.pos_tag(word_tokenize(tweet))\n",
    "    tagged_tweets.append([tag for word,tag in tweet_tag])\n",
    "\n",
    "one_hot_multi = MultiLabelBinarizer()\n",
    "one_hot_multi.fit_transform(tagged_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6509a009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DT', 'IN', 'JJ', 'NN', 'NNP', 'PRP', 'VBG', 'VBP', 'VBZ'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_multi.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd00c96",
   "metadata": {},
   "source": [
    "6.8 : Performing Named-Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39946d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Elon Musk, Twitter, 21B)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Elon Musk offered to buy Twitter using $21B of his own money.\")\n",
    "print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc15bf41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon Musk,PERSON\n",
      "Twitter,PERSON\n",
      "21B,MONEY\n"
     ]
    }
   ],
   "source": [
    "for entity in doc.ents:\n",
    "    print(entity.text , entity.label_,sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb719bb",
   "metadata": {},
   "source": [
    "6.9 : Encoding Text as a Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d3bad21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       "\twith 8 stored elements and shape (3, 8)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = np.array(['I love Brazil. Brazil!',\n",
    "                      'Sweden is best',\n",
    "                      'Germany beats both'])\n",
    "\n",
    "count = CountVectorizer()\n",
    "bag_of_words = count.fit_transform(test_data)\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2c397cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 2, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f79a2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['beats', 'best', 'both', 'brazil', 'germany', 'is', 'love',\n",
       "       'sweden'], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee99a9d",
   "metadata": {},
   "source": [
    "Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9926bf20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_2gram = CountVectorizer(ngram_range=(1,2),\n",
    "                              stop_words=\"english\",\n",
    "                              vocabulary=['brazil'])\n",
    "bag  = count_2gram.fit_transform(test_data)\n",
    "bag.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc300a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brazil': 0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_2gram.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a41254",
   "metadata": {},
   "source": [
    "6.10 : Weighting Word Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51eddb34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 8 stored elements and shape (3, 8)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data = np.array(['I love Brazil. Brazil!',\n",
    "                      'Sweden is best',\n",
    "                      'Germany beats both'])\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "feature_matrix = tfidf.fit_transform(text_data)\n",
    "feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f373ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.89442719, 0.        ,\n",
       "        0.        , 0.4472136 , 0.        ],\n",
       "       [0.        , 0.57735027, 0.        , 0.        , 0.        ,\n",
       "        0.57735027, 0.        , 0.57735027],\n",
       "       [0.57735027, 0.        , 0.57735027, 0.        , 0.57735027,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0784d551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love': 6,\n",
       " 'brazil': 3,\n",
       " 'sweden': 7,\n",
       " 'is': 5,\n",
       " 'best': 1,\n",
       " 'germany': 4,\n",
       " 'beats': 0,\n",
       " 'both': 2}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0281c6c7",
   "metadata": {},
   "source": [
    "6.11 : Using Text Vectors to Calculate Text Similarity in a Search Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ebcd5f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 3 stored elements and shape (1, 8)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data = np.array(['I love Brazil. Brazil!',\n",
    "                      'Sweden is best',\n",
    "                      'Germany beats both'])\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "feature_matrix = tfidf.fit_transform(text_data)\n",
    "\n",
    "text = \"Brazil is the best\"\n",
    "vector = tfidf.transform([text])\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "decf091d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(np.str_('Sweden is best'), np.float64(0.6666666666666666)), (np.str_('I love Brazil. Brazil!'), np.float64(0.5163977794943222)), (np.str_('Germany beats both'), np.float64(0.0))]\n"
     ]
    }
   ],
   "source": [
    "cosine_similarities = linear_kernel(vector,feature_matrix).flatten()\n",
    "related_doc_indicies = cosine_similarities.argsort()[:-10:-1]\n",
    "print([(text_data[i],cosine_similarities[i]) for i in related_doc_indicies])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6be8cd2",
   "metadata": {},
   "source": [
    "6.12 : Using a Sentiment Analysis Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e4801a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\New folder\\Project\\ProjectData\\Data Science\\datascience\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "d:\\New folder\\Project\\ProjectData\\Data Science\\datascience\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ADMIN\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9998020529747009}] [{'label': 'POSITIVE', 'score': 0.9995730519294739}]\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "sentiment_1 = classifier(\"I hate machine learning! It's the absolute worst.\")\n",
    "sentiment_2 = classifier( \"Machine learning is the absolute\"  \"bees knees I love it so much!\")\n",
    "\n",
    "print(sentiment_1,sentiment_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7315e0bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
